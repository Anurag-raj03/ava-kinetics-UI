# #new_app
# import streamlit as st
# import pandas as pd
# import requests
# import json
# import time
# import os
# from dotenv import load_dotenv
# import boto3
# import io

# # ==========================================================
# # üîß Load Environment Variables
# # ==========================================================
# load_dotenv()

# BACKEND_URL = os.getenv("FRONTEND_BACKEND_URL") or "http://fastapi:8000/api/v1/cvat"

# DEFAULT_DB_PARAMS = {
#     "dbname": os.getenv("CVAT_DB_NAME") or "cvat_annotations_db",
#     "user": os.getenv("CVAT_DB_USER") or "admin",
#     "password": os.getenv("CVAT_DB_PASSWORD") or "admin",
#     "host": os.getenv("CVAT_DB_HOST") or "localhost",
#     "port": os.getenv("CVAT_DB_PORT") or "55432",
# }

# DEFAULT_CVAT_HOST = os.getenv("CVAT_HOST") or "http://localhost:8080"
# DEFAULT_CVAT_USER = os.getenv("CVAT_USERNAME") or "Strawhat03"
# DEFAULT_CVAT_PASS = os.getenv("CVAT_PASSWORD") or "Test@123"
# DEFAULT_S3_BUCKET = os.getenv("AWS_STORAGE_BUCKET_NAME") or "ava-kine-data"

# # ==========================================================
# # ‚ôªÔ∏è Helper Functions
# # ==========================================================
# def rerun(force: bool = True):
#     if force and "fetch_projects_api" in st.session_state:
#         del st.session_state["fetch_projects_api"]
#     if hasattr(st, "rerun"):
#         st.rerun()
#     elif hasattr(st, "experimental_rerun"):
#         st.experimental_rerun()

# def call_api(method, endpoint, json_data=None, expect_file=False):
#     url = f"{BACKEND_URL}{endpoint}"
#     try:
#         response = requests.request(method, url, json=json_data, stream=expect_file)
#         response.raise_for_status()
#         if expect_file:
#             return response
#         try:
#             return response.json()
#         except requests.exceptions.JSONDecodeError:
#             return {"message": "Success", "content": response.text}
#     except requests.exceptions.HTTPError as e:
#         detail = "Unknown error"
#         try:
#             detail = e.response.json().get("detail", e.response.text)
#         except:
#             detail = e.response.text
#         st.error(f"API Error ({e.response.status_code}): {detail}")
#         return None
#     except requests.exceptions.RequestException as e:
#         st.error(f"Connection Error: Could not connect to backend at {BACKEND_URL}.")
#         st.exception(e)
#         return None

# @st.cache_data(show_spinner="Fetching projects...")
# def fetch_projects_api(params):
#     return call_api("POST", "/qc/projects", json_data=params)

# def load_pending_tasks(db_params, project_id):
#     tasks = call_api("POST", f"/qc/pending_tasks/{project_id}", json_data=db_params)
#     st.session_state["task_data"] = tasks or []

# def list_s3_batches(bucket_name):
#     s3 = boto3.client("s3")
#     paginator = s3.get_paginator("list_objects_v2")
#     batches = set()
#     for page in paginator.paginate(Bucket=bucket_name, Delimiter="/"):
#         for prefix in page.get("CommonPrefixes", []):
#             batches.add(prefix.get("Prefix").rstrip("/"))
#     return sorted(list(batches))

# def list_s3_clips(bucket_name, batch_name):
#     s3 = boto3.client("s3")
#     prefix = f"{batch_name}/frames/"
#     clips = []
#     paginator = s3.get_paginator("list_objects_v2")
#     for page in paginator.paginate(Bucket=bucket_name, Prefix=prefix):
#         for obj in page.get("Contents", []):
#             if obj["Key"].endswith(".zip"):
#                 clips.append(obj["Key"].split("/")[-1])
#     return sorted(clips)

# def list_s3_manifests(bucket_name, batch_name):
#     s3 = boto3.client("s3")
#     prefix = f"{batch_name}/manifests/"
#     manifests = []
#     paginator = s3.get_paginator("list_objects_v2")
#     for page in paginator.paginate(Bucket=bucket_name, Prefix=prefix):
#         for obj in page.get("Contents", []):
#             if obj["Key"].endswith(".json"):
#                 manifests.append(obj["Key"].split("/")[-1])
#     return sorted(manifests)

# # ==========================================================
# # üñ•Ô∏è Streamlit UI Setup
# # ==========================================================
# st.set_page_config(page_title="AVA-Kinetics Pipeline UI", layout="wide")
# st.title("üöÄ Integrated AVA-Kinetics Pipeline UI")
# st.markdown("---")

# tab_qc, tab_creator = st.tabs(["‚úÖ QC & Dataset Generator", "üöÄ CVAT Task Creator (S3)"])

# # Sidebar Config
# st.sidebar.header("‚öôÔ∏è DB Configuration (QC/Generator)")
# db_params = {
#     "dbname": st.sidebar.text_input("DB Name", DEFAULT_DB_PARAMS["dbname"]),
#     "user": st.sidebar.text_input("DB User", DEFAULT_DB_PARAMS["user"]),
#     "password": st.sidebar.text_input("DB Password", DEFAULT_DB_PARAMS["password"], type="password"),
#     "host": st.sidebar.text_input("DB Host", DEFAULT_DB_PARAMS["host"]),
#     "port": st.sidebar.text_input("DB Port", DEFAULT_DB_PARAMS["port"]),
# }

# st.sidebar.markdown("---")
# st.sidebar.header("‚öôÔ∏è CVAT/S3 Config (Task Creator)")
# cvat_host = st.sidebar.text_input("CVAT Host URL", DEFAULT_CVAT_HOST)
# cvat_user = st.sidebar.text_input("CVAT Username", DEFAULT_CVAT_USER)
# cvat_pass = st.sidebar.text_input("CVAT Password", DEFAULT_CVAT_PASS, type="password")
# s3_bucket_name_sidebar = st.sidebar.text_input("S3 Bucket Name", DEFAULT_S3_BUCKET)

# # ==========================================================
# # üßæ QC & Dataset Generator Tab
# # ==========================================================
# with tab_qc:
#     st.header("1Ô∏è‚É£ Quality Control Approval")
#     projects = fetch_projects_api(db_params) or {}
#     project_options = [f"{pid}: {pname}" for pid, pname in projects.items()]
#     selected_project_str = st.selectbox("Select Project for QC", project_options)

#     if selected_project_str:
#         project_id = int(selected_project_str.split(":")[0])
#         st.session_state["qc_project_select_id"] = project_id
#         load_pending_tasks(db_params, project_id)
#         tasks = st.session_state.get("task_data", [])
#         st.subheader(f"Pending Tasks ({len(tasks)})")
#         if tasks:
#             df_tasks = pd.DataFrame(tasks)
#             st.dataframe(df_tasks)
#             if st.button("‚úÖ Approve All Pending Tasks"):
#                 resp = call_api("POST", f"/qc/approve_all_pending/{project_id}", json_data=db_params)
#                 if resp:
#                     st.success(f"Approved {resp['approved_count']} tasks.")
#                     load_pending_tasks(db_params, project_id)
#         else:
#             st.info("No pending tasks found for this project.")

#     st.markdown("---")
#     st.header("2Ô∏è‚É£ Generate Final Dataset")
#     s3_bucket_name_qc = st.text_input("S3 Bucket Name for Manifest", value=s3_bucket_name_sidebar, key="qc_bucket")
#     selected_batch_qc = st.text_input("Batch Name", "factory_batch_01", key="qc_batch")
#     manifests = []
#     if s3_bucket_name_qc and selected_batch_qc:
#         try:
#             manifests = list_s3_manifests(s3_bucket_name_qc, selected_batch_qc)
#         except Exception as e:
#             st.error(f"Failed to list manifests: {e}")

#     manifest_path_str = None
#     if manifests:
#         manifest_choice = st.selectbox("Select Manifest", manifests)
#         manifest_path_str = f"s3://{s3_bucket_name_qc}/{selected_batch_qc}/manifests/{manifest_choice}"

#     default_output_filename = f"project_{st.session_state.get('qc_project_select_id', '0')}_ava_kinetics_dataset.csv"
#     output_filename = st.text_input("Output CSV Filename", default_output_filename)

#     def generate_dataset():
#         if not st.session_state.get("qc_project_select_id"):
#             st.error("‚ùå Select a project first.")
#         elif not manifest_path_str:
#             st.error("‚ùå Select a manifest first.")
#         else:
#             payload = {
#                 "db_params": db_params,
#                 "project_id": st.session_state.get("qc_project_select_id"),
#                 "manifest_path": manifest_path_str,
#                 "output_filename": output_filename,
#             }
#             with st.spinner("Generating dataset..."):
#                 try:
#                     response = call_api("POST", "/qc/generate_dataset", json_data=payload, expect_file=True)
#                     if response is None:
#                         return
#                     try:
#                         error_json = response.json()
#                         st.error(f"‚ùå Download failed. Received JSON message instead of file: {error_json.get('message', 'Unknown JSON error')}")
#                         return
#                     except json.JSONDecodeError:
#                         pass
#                     csv_bytes = io.BytesIO(response.content)
#                     csv_bytes.seek(0)
#                     st.success(f"‚úÖ Dataset generated: `{output_filename}`")
#                     st.download_button(
#                         label="‚¨áÔ∏è Download Dataset CSV",
#                         data=csv_bytes,
#                         file_name=output_filename,
#                         mime="text/csv"
#                     )
#                 except Exception as e:
#                     st.error(f"Failed to process dataset response: {e}")

#     if st.button("üìä Generate Dataset"):
#         generate_dataset()

# # ==========================================================
# # üß± CVAT Task Creator Tab
# # ==========================================================
# with tab_creator:
#     st.header("üß± CVAT Task Creator (S3 Direct)")
#     st.markdown("This tool connects to CVAT, lists clips from S3, and creates tasks for a project.")
#     st.subheader("Step 1: Select S3 Batch")
#     s3_bucket_name = st.text_input("S3 Bucket Name", value=s3_bucket_name_sidebar, key="creator_bucket")
#     batch_name = None
#     if s3_bucket_name:
#         try:
#             batches = list_s3_batches(s3_bucket_name)
#             if batches:
#                 batch_name = st.selectbox("Select Batch", batches, key="creator_batch_select")
#             else:
#                 st.warning("‚ö†Ô∏è No batches found in the bucket.")
#         except Exception as e:
#             st.error(f"Failed to list S3 batches: {e}")

#     clip_names_input = ""
#     if batch_name:
#         try:
#             clips = list_s3_clips(s3_bucket_name, batch_name)
#             if clips:
#                 clip_names_input = st.text_area("Clip ZIP File Names (comma-separated)", value=",".join(clips), key="creator_clips")
#         except Exception as e:
#             st.error(f"Failed to list clips: {e}")

#     st.subheader("Step 3: Assign Annotators")
#     annotator_input = st.text_area("Annotators (comma-separated)", value="", key="creator_annotators")
#     annotators = [a.strip() for a in annotator_input.split(",") if a.strip()]

#     st.subheader("Step 4: Create CVAT Project & Tasks")
#     project_name = st.text_input("New CVAT Project Name", f"S3_Project_{int(time.time())}", key="creator_project_name")

#     if st.button("üöÄ Create Project from S3"):
#         if not s3_bucket_name:
#             st.error("‚ùå Enter S3 Bucket Name.")
#         elif not batch_name:
#             st.error("‚ùå Select a batch from S3.")
#         elif not all([cvat_host, cvat_user, cvat_pass]):
#             st.error("‚ùå Fill all CVAT credentials.")
#         else:
#             clip_list = [c.strip() for c in clip_names_input.split(",") if c.strip()]
#             payload = {
#                 "project_name": project_name,
#                 "batch_name": batch_name,
#                 "s3_bucket": s3_bucket_name,
#                 "host": cvat_host,
#                 "username": cvat_user,
#                 "password": cvat_pass,
#                 "clips": clip_list,
#                 "annotators": annotators
#             }
#             with st.spinner("Creating CVAT project and tasks from S3..."):
#                 resp = call_api("POST", "/create_project_and_tasks_s3", json_data=payload)
#                 if resp:
#                     st.success("‚úÖ Project and tasks created successfully!")
#                     st.json(resp)
#                     time.sleep(2.0)
#                     st.session_state["needs_project_refresh"] = True
#                     st.session_state["s3_clip_names"] = ""
#                     time.sleep(0.1)
#                     rerun()




import streamlit as st
import pandas as pd
import requests
import json
import time
import os
from dotenv import load_dotenv
import boto3
import io

load_dotenv()

BACKEND_URL = os.getenv("FRONTEND_BACKEND_URL") or "http://fastapi:8000/api/v1/cvat"

DEFAULT_DB_PARAMS = {
    "dbname": os.getenv("CVAT_DB_NAME") or "cvat_annotations_db",
    "user": os.getenv("CVAT_DB_USER") or "admin",
    "password": os.getenv("CVAT_DB_PASSWORD") or "admin",
    "host": os.getenv("CVAT_DB_HOST") or "localhost",
    "port": os.getenv("CVAT_DB_PORT") or "55432",
}

DEFAULT_CVAT_HOST = os.getenv("CVAT_HOST") or "http://localhost:8080"
DEFAULT_CVAT_USER = os.getenv("CVAT_USERNAME") or "Strawhat03"
DEFAULT_CVAT_PASS = os.getenv("CVAT_PASSWORD") or "Test@123"
DEFAULT_S3_BUCKET = os.getenv("AWS_STORAGE_BUCKET_NAME") or "ava-kine-data"

def rerun(force: bool = True):
    if hasattr(st, "rerun"):
        st.rerun()
    elif hasattr(st, "experimental_rerun"):
        st.experimental_rerun()

def call_api(method, endpoint, json_data=None, expect_file=False):
    url = f"{BACKEND_URL}{endpoint}"
    try:
        response = requests.request(method, url, json=json_data, stream=expect_file)
        response.raise_for_status()
        if expect_file:
            return response
        try:
            return response.json()
        except requests.exceptions.JSONDecodeError:
            return {"message": "Success", "content": response.text}
    except requests.exceptions.RequestException as e:
        detail = str(e)
        if hasattr(e, 'response') and e.response is not None:
            try:
                detail = e.response.json().get("detail", e.response.text)
            except:
                detail = e.response.text
        st.error(f"Connection/API Error: {detail}")
        return None

@st.cache_data(show_spinner="Fetching projects...")
def fetch_projects_api(params):
    return call_api("POST", "/qc/projects", json_data=params)

def load_pending_tasks(db_params, project_id):
    tasks = call_api("POST", f"/qc/pending_tasks/{project_id}", json_data=db_params)
    st.session_state["task_data"] = tasks or []

def list_s3_batches(bucket_name):
    s3 = boto3.client("s3")
    paginator = s3.get_paginator("list_objects_v2")
    batches = set()
    for page in paginator.paginate(Bucket=bucket_name, Delimiter="/"):
        for prefix in page.get("CommonPrefixes", []):
            batches.add(prefix.get("Prefix").rstrip("/"))
    return sorted(list(batches))

def list_s3_clips(bucket_name, batch_name):
    s3 = boto3.client("s3")
    prefix = f"{batch_name}/frames/"
    clips = []
    paginator = s3.get_paginator("list_objects_v2")
    for page in paginator.paginate(Bucket=bucket_name, Prefix=prefix):
        for obj in page.get("Contents", []):
            if obj["Key"].endswith(".zip"):
                clips.append(obj["Key"].split("/")[-1])
    return sorted(clips)

def list_s3_manifests(bucket_name, batch_name):
    s3 = boto3.client("s3")
    prefix = f"{batch_name}/manifests/"
    manifests = []
    paginator = s3.get_paginator("list_objects_v2")
    for page in paginator.paginate(Bucket=bucket_name, Prefix=prefix):
        for obj in page.get("Contents", []):
            if obj["Key"].endswith(".json"):
                manifests.append(obj["Key"].split("/")[-1])
    return sorted(manifests)

st.set_page_config(page_title="AVA-Kinetics Pipeline UI", layout="wide")
st.title("üöÄ Integrated AVA-Kinetics Pipeline UI")
st.markdown("---")

tab_qc, tab_creator = st.tabs(["‚úÖ QC & Dataset Generator", "üöÄ CVAT Task Creator (S3)"])

st.sidebar.header("‚öôÔ∏è DB Configuration (QC/Generator)")
db_params = {
    "dbname": st.sidebar.text_input("DB Name", DEFAULT_DB_PARAMS["dbname"]),
    "user": st.sidebar.text_input("DB User", DEFAULT_DB_PARAMS["user"]),
    "password": st.sidebar.text_input("DB Password", DEFAULT_DB_PARAMS["password"], type="password"),
    "host": st.sidebar.text_input("DB Host", DEFAULT_DB_PARAMS["host"]),
    "port": st.sidebar.text_input("DB Port", DEFAULT_DB_PARAMS["port"]),
}

st.sidebar.markdown("---")
st.sidebar.header("‚öôÔ∏è CVAT/S3 Config (Task Creator)")
cvat_host = st.sidebar.text_input("CVAT Host URL", DEFAULT_CVAT_HOST)
cvat_user = st.sidebar.text_input("CVAT Username", DEFAULT_CVAT_USER)
cvat_pass = st.sidebar.text_input("CVAT Password", DEFAULT_CVAT_PASS, type="password")
s3_bucket_name_sidebar = st.sidebar.text_input("S3 Bucket Name", DEFAULT_S3_BUCKET)

with tab_qc:
    st.header("1Ô∏è‚É£ Quality Control Approval")
    st.session_state.setdefault("needs_project_refresh", False)
    st.session_state.setdefault("task_data", [])
    st.session_state.setdefault("qc_project_select_id", None)

    if st.session_state["needs_project_refresh"]:
        fetch_projects_api.clear()
        st.session_state["needs_project_refresh"] = False

    projects = fetch_projects_api(db_params) or {}
    project_options = [f"{pid}: {pname}" for pid, pname in projects.items()]

    if not project_options:
        st.warning("No projects found in the database. Check your DB connection or create a project.")
        selected_project_str = None
    else:
        selected_project_str = st.selectbox("Select Project for QC", project_options)

    if selected_project_str:
        project_id = int(selected_project_str.split(":")[0])
        if project_id != st.session_state.get("qc_project_select_id"):
            st.session_state["qc_project_select_id"] = project_id
            load_pending_tasks(db_params, project_id)

        tasks = st.session_state.get("task_data", [])
        st.subheader(f"Pending Tasks ({len(tasks)})")
        if tasks:
            st.dataframe(pd.DataFrame(tasks))
            if st.button("‚úÖ Approve All Pending Tasks"):
                resp = call_api("POST", f"/qc/approve_all_pending/{project_id}", json_data=db_params)
                if resp:
                    st.success(f"Approved {resp['approved_count']} tasks.")
                    load_pending_tasks(db_params, project_id)
                    rerun()
        else:
            st.info("No pending tasks found.")

    st.markdown("---")
    st.header("2Ô∏è‚É£ Generate Final Dataset")
    s3_bucket_name_qc = st.text_input("S3 Bucket Name for Manifest", value=s3_bucket_name_sidebar, key="qc_bucket")
    selected_batch_qc = st.text_input("Batch Name", "factory_batch_01", key="qc_batch")
    manifests = list_s3_manifests(s3_bucket_name_qc, selected_batch_qc) if s3_bucket_name_qc and selected_batch_qc else []

    manifest_path_str = None
    if manifests:
        manifest_choice = st.selectbox("Select Manifest", manifests)
        manifest_path_str = f"s3://{s3_bucket_name_qc}/{selected_batch_qc}/manifests/{manifest_choice}"

    default_output_filename = f"project_{st.session_state.get('qc_project_select_id', '0')}_ava_kinetics_dataset.csv"
    output_filename = st.text_input("Output CSV Filename", default_output_filename)

    if st.button("üìä Generate Dataset"):
        if not st.session_state.get("qc_project_select_id"):
            st.error("‚ùå Select a project first.")
        elif not manifest_path_str:
            st.error("‚ùå Select a manifest first.")
        else:
            payload = {
                "db_params": db_params,
                "project_id": st.session_state.get("qc_project_select_id"),
                "manifest_path": manifest_path_str,
                "output_filename": output_filename,
            }
            with st.spinner("Generating dataset..."):
                response = call_api("POST", "/qc/generate_dataset", json_data=payload, expect_file=True)
                if response:
                    try:
                        error_json = response.json()
                        st.error(f"‚ùå Download failed. Received JSON error: {error_json.get('detail', 'Unknown error')}")
                        return
                    except json.JSONDecodeError:
                        csv_bytes = io.BytesIO(response.content)
                        csv_bytes.seek(0)
                        st.success(f"‚úÖ Dataset generated: `{output_filename}`")
                        st.download_button("‚¨áÔ∏è Download Dataset CSV", data=csv_bytes, file_name=output_filename, mime="text/csv")

with tab_creator:
    st.header("üß± CVAT Task Creator (S3 Direct)")
    st.session_state.setdefault("s3_clip_names", "")

    s3_bucket_name = st.text_input("S3 Bucket Name", value=s3_bucket_name_sidebar, key="creator_bucket_input")
    batches = list_s3_batches(s3_bucket_name) if s3_bucket_name else []
    batch_name = st.selectbox("Select Batch", batches) if batches else None
    if not batches and s3_bucket_name:
        st.warning("No batches (subfolders) found in the S3 bucket.")

    clip_names_input = ""
    if batch_name:
        clips = list_s3_clips(s3_bucket_name, batch_name)
        clip_names_input = st.text_area("Clip ZIP File Names (comma-separated)", value=",".join(clips), key="creator_clips")

    annotator_input = st.text_area("Annotators (comma-separated)", value="", key="creator_annotators")
    annotators = [a.strip() for a in annotator_input.split(",") if a.strip()]

    project_name = st.text_input("New CVAT Project Name", f"S3_Project_{int(time.time())}", key="creator_project_name")

    if st.button("üöÄ Create Project from S3"):
        if not s3_bucket_name or not batch_name or not all([cvat_host, cvat_user, cvat_pass]):
            st.error("‚ùå Fill all required fields.")
        else:
            clip_list = [c.strip() for c in clip_names_input.split(",") if c.strip()]
            payload = {
                "project_name": project_name,
                "batch_name": batch_name,
                "s3_bucket": s3_bucket_name,
                "host": cvat_host,
                "username": cvat_user,
                "password": cvat_pass,
                "clips": clip_list,
                "annotators": annotators
            }
            with st.spinner("Creating CVAT project and tasks..."):
                resp = call_api("POST", "/create_project_and_tasks_s3", json_data=payload)
                if resp:
                    st.success("‚úÖ Project and tasks created successfully!")
                    st.json(resp)
                    time.sleep(2)
                    st.session_state["needs_project_refresh"] = True
                    time.sleep(0.5)
                    rerun()
